{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from notebooks import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=sqlContext.read.load('file:///home/cloudera/Downloads/big-data-4/minute_weather.csv',format='com.databricks.spark.csv',\n",
    "                       header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowID</th>\n",
       "      <th>hpwren_timestamp</th>\n",
       "      <th>air_pressure</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>avg_wind_direction</th>\n",
       "      <th>avg_wind_speed</th>\n",
       "      <th>max_wind_direction</th>\n",
       "      <th>max_wind_speed</th>\n",
       "      <th>min_wind_direction</th>\n",
       "      <th>min_wind_speed</th>\n",
       "      <th>rain_accumulation</th>\n",
       "      <th>rain_duration</th>\n",
       "      <th>relative_humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-09-10 00:00:49</td>\n",
       "      <td>912.3</td>\n",
       "      <td>64.76</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-09-10 00:01:49</td>\n",
       "      <td>912.3</td>\n",
       "      <td>63.86</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>215.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-09-10 00:02:49</td>\n",
       "      <td>912.3</td>\n",
       "      <td>64.22</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rowID    hpwren_timestamp  air_pressure  air_temp  avg_wind_direction  \\\n",
       "0      0 2011-09-10 00:00:49         912.3     64.76                97.0   \n",
       "1      1 2011-09-10 00:01:49         912.3     63.86               161.0   \n",
       "2      2 2011-09-10 00:02:49         912.3     64.22                77.0   \n",
       "\n",
       "   avg_wind_speed  max_wind_direction  max_wind_speed  min_wind_direction  \\\n",
       "0             1.2               106.0             1.6                85.0   \n",
       "1             0.8               215.0             1.5                43.0   \n",
       "2             0.7               143.0             1.2               324.0   \n",
       "\n",
       "   min_wind_speed  rain_accumulation  rain_duration  relative_humidity  \n",
       "0             1.0                NaN            NaN               60.5  \n",
       "1             0.2                0.0            0.0               39.9  \n",
       "2             0.3                0.0            0.0               43.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df.take(3),columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rowID: integer (nullable = true)\n",
      " |-- hpwren_timestamp: timestamp (nullable = true)\n",
      " |-- air_pressure: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- avg_wind_direction: double (nullable = true)\n",
      " |-- avg_wind_speed: double (nullable = true)\n",
      " |-- max_wind_direction: double (nullable = true)\n",
      " |-- max_wind_speed: double (nullable = true)\n",
      " |-- min_wind_direction: double (nullable = true)\n",
      " |-- min_wind_speed: double (nullable = true)\n",
      " |-- rain_accumulation: double (nullable = true)\n",
      " |-- rain_duration: double (nullable = true)\n",
      " |-- relative_humidity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1587257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=df.filter(df.rowID%10==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158726"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowID</th>\n",
       "      <th>hpwren_timestamp</th>\n",
       "      <th>air_pressure</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>avg_wind_direction</th>\n",
       "      <th>avg_wind_speed</th>\n",
       "      <th>max_wind_direction</th>\n",
       "      <th>max_wind_speed</th>\n",
       "      <th>min_wind_direction</th>\n",
       "      <th>min_wind_speed</th>\n",
       "      <th>relative_humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-09-10 00:00:49</td>\n",
       "      <td>912.3</td>\n",
       "      <td>64.76</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2011-09-10 00:10:49</td>\n",
       "      <td>912.3</td>\n",
       "      <td>62.24</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>38.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>2011-09-10 00:20:49</td>\n",
       "      <td>912.2</td>\n",
       "      <td>63.32</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>58.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rowID    hpwren_timestamp  air_pressure  air_temp  avg_wind_direction  \\\n",
       "0      0 2011-09-10 00:00:49         912.3     64.76                97.0   \n",
       "1     10 2011-09-10 00:10:49         912.3     62.24               144.0   \n",
       "2     20 2011-09-10 00:20:49         912.2     63.32               100.0   \n",
       "\n",
       "   avg_wind_speed  max_wind_direction  max_wind_speed  min_wind_direction  \\\n",
       "0             1.2               106.0             1.6                85.0   \n",
       "1             1.2               167.0             1.8               115.0   \n",
       "2             2.0               122.0             2.5                91.0   \n",
       "\n",
       "   min_wind_speed  relative_humidity  \n",
       "0             1.0               60.5  \n",
       "1             0.6               38.5  \n",
       "2             1.5               58.3  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df1.take(3),columns=df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1=df1.drop('rain_accumulation').drop('rain_duration')\n",
    "\n",
    "df1=df1.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage=[]\n",
    "feature_columns=[\n",
    " 'air_pressure',\n",
    " 'air_temp',\n",
    " 'avg_wind_direction',\n",
    " 'avg_wind_speed',\n",
    " 'max_wind_direction',\n",
    " 'max_wind_speed',\n",
    " 'relative_humidity']\n",
    "\n",
    "assembler=VectorAssembler(inputCols=feature_columns,outputCol='feature')\n",
    "stage=stage+[assembler]\n",
    "\n",
    "scaler=StandardScaler(inputCol='feature',outputCol='feature_scale',withStd=True,withMean=True)\n",
    "stage=stage+[scaler]\n",
    "\n",
    "\n",
    "\n",
    "#Add custom transformer \n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "class ColumnKeeper(Transformer):\n",
    "    def __init__(self, column_keep):\n",
    "        #super(ColumnKeeper, self).__init__()\n",
    "        self.column_keep = column_keep\n",
    "    def _transform(self, df):\n",
    "        df = df.select(self.column_keep)\n",
    "        return df\n",
    "keeper=ColumnKeeper('feature_scale')\n",
    "stage=stage+[keeper]\n",
    "\n",
    "modeler=KMeans(k=3,seed=613,featuresCol='feature_scale',predictionCol='cluster')\n",
    "stage=stage+[modeler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=stage)\n",
    "model=pipeline.fit(df1)\n",
    "\n",
    "df2=model.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(feature_scale=DenseVector([-1.4846, 0.2454, -0.6839, -0.7656, -0.6215, -0.7444, 0.4923]), cluster=1),\n",
       " Row(feature_scale=DenseVector([-1.4846, 0.0325, -0.1906, -0.7656, 0.0383, -0.6617, -0.3471]), cluster=0),\n",
       " Row(feature_scale=DenseVector([-1.5173, 0.1237, -0.6524, -0.3768, -0.4485, -0.3723, 0.4084]), cluster=2)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSS elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3=assembler.transform(df1)\n",
    "fit=scaler.fit(df3)\n",
    "df3=fit.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for cluster size 3 \n",
      "......................WSSE = 310428.7431401998 \n",
      "Training for cluster size 5 \n",
      "......................WSSE = 274560.65811044944 \n",
      "Training for cluster size 7 \n",
      "......................WSSE = 245261.85723243063 \n",
      "Training for cluster size 9 \n",
      "......................WSSE = 227025.58599483027 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[310428.7431401998, 274560.65811044944, 245261.85723243063, 227025.58599483027]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=df3.select('features')\n",
    "df3.persist()  #keep data in memory \n",
    "cluster=range(3,10,2)\n",
    "utils.elbow(df3,cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.ml.evaluation' has no attribute 'ClusteringEvaluator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-0ce2b33c0952>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mevaluator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClusteringEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msilhouette\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyspark.ml.evaluation' has no attribute 'ClusteringEvaluator'"
     ]
    }
   ],
   "source": [
    "# required Spark 2.4 above\n",
    "\n",
    "from pyspark.ml import evaluation \n",
    "evaluator = evaluation.ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
